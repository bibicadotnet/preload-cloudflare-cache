name: Simple Sitemap Crawler

on:
  workflow_dispatch:

jobs:
  crawl:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - uses: actions/setup-node@v4
      with:
        node-version: '18'
        
    - name: Install
      run: npm install puppeteer
        
    - name: Create crawler
      run: |
        cat > crawler.js << 'EOF'
        const puppeteer = require('puppeteer');

        async function crawl() {
          const browser = await puppeteer.launch({
            headless: true,
            args: ['--no-sandbox', '--disable-setuid-sandbox']
          });
          
          const page = await browser.newPage();
          
          try {
            // Get sitemap
            await page.goto('https://bibica.net/sitemap.xml');
            const content = await page.content();
            
            // Extract URLs with regex (simple & works)
            const urls = content.match(/<loc>(.*?)<\/loc>/g)
              ?.map(match => match.replace(/<\/?loc>/g, ''))
              || [];
            
            console.log(`Found ${urls.length} URLs`);
            
            const results = [];
            
            // Visit each URL
            for (let i = 0; i < Math.min(urls.length, 20); i++) { // Limit to 20 for testing
              const url = urls[i];
              console.log(`${i+1}/${urls.length}: ${url}`);
              
              try {
                const response = await page.goto(url, { timeout: 15000 });
                const title = await page.title();
                
                results.push({
                  url: url,
                  status: response.status(),
                  title: title
                });
                
                console.log(`✓ ${response.status()} - ${title}`);
                
              } catch (error) {
                console.log(`✗ Error: ${error.message}`);
                results.push({ url: url, error: error.message });
              }
              
              await page.waitForTimeout(500); // Wait 0.5s between requests
            }
            
            console.log('\n=== RESULTS ===');
            console.log(JSON.stringify(results, null, 2));
            
          } finally {
            await browser.close();
          }
        }

        crawl();
        EOF
        
    - name: Run crawler
      run: node crawler.js
